<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://n-reeves.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://n-reeves.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-27T05:37:59+00:00</updated><id>https://n-reeves.github.io/feed.xml</id><title type="html">blank</title><subtitle>Professional homepage for Nick Reeves. </subtitle><entry><title type="html">Neural Speech Enhancer - Architecture, Training, and Post-Processing</title><link href="https://n-reeves.github.io/blog/2025/speech-enhancement-network/" rel="alternate" type="text/html" title="Neural Speech Enhancer - Architecture, Training, and Post-Processing"/><published>2025-01-24T01:59:00+00:00</published><updated>2025-01-24T01:59:00+00:00</updated><id>https://n-reeves.github.io/blog/2025/speech-enhancement-network</id><content type="html" xml:base="https://n-reeves.github.io/blog/2025/speech-enhancement-network/"><![CDATA[<hr/> <h2 id="introduction">Introduction</h2> <p>Speech enhancement aims to improve the audibility and clarity of speech in noisy or reverberant environments. This project explores a neural network-based approach for reducing background noise and echo while enhancing speech quality. The system leverages a Complex U-Net architecture trained to process noisy and reverberant audio, focusing on single-channel, two-speaker scenarios.</p> <p>The <a href="https://n-reeves.github.io/projects/Neural-Speech-Enhancement/">project page</a> on this topic contains an interactive demo that allows you to test the model yourself.</p> <p><em>Disclaimer on generative AI use:</em> This document was created with the help of ChatGPT, which summarized key content from a more comprehensive paper I wrote. I have verified the accuracy of the information listed here and edited the content where appropriate.</p> <hr/> <h2 id="background">Background</h2> <ul> <li><strong>Audio Signal</strong>: A time-domain representation of sound that can be transformed into frequency components using techniques like the Short-Time Fourier Transform (STFT).</li> <li><strong>STFT</strong>: A method that splits an audio signal into overlapping frames, applying the Fourier transform to each to yield time-frequency representations.</li> <li><strong>STFT Coefficients</strong>: Complex numbers representing the amplitude (magnitude) and phase of audio signals at specific time-frequency bins.</li> <li><strong>Convolutional Neural Networks (CNNs)</strong>: Neural networks designed to extract features from grid-like data, such as images or spectrograms.</li> <li><strong>Spectrograms</strong>: Visual representations of the frequency content of a signal over time, derived from STFT magnitudes.</li> <li><strong>U-Net</strong>: An encoder-decoder network architecture with skip connections, commonly used for image segmentation and extended here for speech enhancement.</li> <li><strong>Complex Convolutions</strong>: Extensions of convolution operations to the complex domain, enabling models to process both magnitude and phase components.</li> </ul> <hr/> <h2 id="demo">Demo</h2> <p>Here is a preview of the system’s capability. There are clear aesthetic issues with the output, however it appears that background noise is being supressed and the speech content is largely preserved. More details on how this was achieved follow.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <label for="input-audio-example">Input Audio Example</label> <figure> <audio src="/assets/audio/quiet-speech-loud-drone_in.wav" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <label for="output-audio-example">Output Audio Example</label> <figure> <audio src="/assets/audio/quiet-speech-loud-drone_out.wav" controls=""/> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/speech-enhance/speech-enhance-example-480.webp 480w,/assets/img/speech-enhance/speech-enhance-example-800.webp 800w,/assets/img/speech-enhance/speech-enhance-example-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/speech-enhance/speech-enhance-example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="experiment-design">Experiment Design</h2> <h3 id="objective">Objective</h3> <p>To enhance speech clarity by reducing background noise and reverberation in two-speaker audio mixtures.</p> <h3 id="dataset">Dataset</h3> <ol> <li><strong>LibriMix Dataset</strong>: <ul> <li>8,000 samples of fully overlapping, 8 kHz two-speaker mixtures.</li> <li>Divided into 75% training, 12.5% validation, and 12.5% testing subsets, ensuring no speaker overlap between train and test sets.</li> <li>Additive noise was applied for augmentation.</li> </ul> </li> <li><strong>MIT Impulse Response Dataset</strong>: <ul> <li>271 impulse responses simulating real-world reverberant environments.</li> <li>Partitioned into 70% training, 15% validation, and 15% testing splits.</li> </ul> </li> </ol> <h3 id="data-augmentation">Data Augmentation</h3> <ul> <li>Random cropping was applied to training data to introduce noise variations.</li> <li>Reverberation was added to mixtures using impulse responses from the MIT dataset.</li> </ul> <hr/> <h3 id="loss-function">Loss Function</h3> <p>The loss function minimizes the difference between the predicted and target STFT coefficients:</p> \[\mathcal{L} = \sum_{t,f} \left( | \hat{R}_{t,f} - R_{t,f} |^2 + | \hat{I}_{t,f} - I_{t,f} |^2 \right)\] <p>Where \(\hat{R}_{t,f}\) and \(\hat{I}_{t,f}\) are the predicted real and imaginary components, and \(R_{t,f}\) and \(I_{t,f}\) are the target components at time \(t\) and frequency \(f\).</p> <hr/> <h3 id="evaluation-metrics">Evaluation Metrics</h3> <ul> <li><strong>Signal-to-Distortion Ratio (SDR)</strong>: Measures the quality of the enhanced signal relative to the ground truth.</li> </ul> \[\text{SDR} = 10 \log_{10} \frac{\|s_{\text{target}}\|^2}{\|s_{\text{error}}\|^2}\] <p>Where \(s_{\text{target}}\) is the ground truth signal, and \(s_{\text{error}}\) is the difference between the ground truth and the predicted signal.</p> <ul> <li><strong>Perceptual Evaluation of Speech Quality (PESQ)</strong>: Quantifies speech quality as perceived by humans.</li> </ul> \[\text{PESQ} = f_{\text{pesq}}(s_{\text{enhanced}}, s_{\text{reference}})\] <p>Where \(f_{\text{pesq}}\) is the PESQ evaluation function, \(s_{\text{enhanced}}\) is the enhanced signal, and \(s_{\text{reference}}\) is the reference signal.</p> <p>The final network was chosen based on the average difference between the PESQ scores of the network’s output compared to the clean, unaugmented input, and the PESQ scores of the noisy reverberant input audio compared to the clean input.</p> <h2 id="architecture">Architecture</h2> <p>In audio processing, neural architectures are often designed to work with spectrograms, as this simplifies the handling of complex numbers. It also enables the use of existing image processing techniques, such as convolutional neural networks (CNNs), without issues. However, this approach requires discarding phase information, which can lead to challenges depending on the task.</p> <ul> <li>The network cannot leverage phase information during training.</li> <li>If the system is designed to work with spectrograms (e.g., through masking), the output will be real-valued.</li> </ul> <p>In the context of speech enhancement, I aimed to create a mask that could be applied directly to the output. By applying a mask rather than synthesizing an enhanced signal, the stability of the network is improved. Additionally, when a network is trained to generate a magnitude spectrogram, phase information must be approximated using techniques like Griffin-Lim, or a different system must be employed to convert the real-valued coefficients back to the signal domain.</p> <h3 id="complex-convolutional-layers">Complex Convolutional Layers</h3> <p>A complex convolutional filter \(W = A + iB\) applied to a complex input \(h = x + yi\) produces:</p> \[Wh = (A * x - B * y) + i(B * x + A * y)\] <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/speech-enhance/complexconv-480.webp 480w,/assets/img/speech-enhance/complexconv-800.webp 800w,/assets/img/speech-enhance/complexconv-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/speech-enhance/complexconv.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Real vs. Complex Convolution" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <label for="output-audio-example">Diagram from Choi, H.-S. et al. (2019) (author redacted paper) </label> </div> </div> <p>This idea was proposed by Trabelsi, C. et al. (2018), who also extend the logic to activation functions and different normalization techniques.</p> <h3 id="complex-u-net">Complex U-Net</h3> <p>The architecture consists of an encoder-decoder network with complex convolutions:</p> <ol> <li><strong>Encoder</strong>: Sequential layers of complex convolutions and downsampling that capture compact representations of the input spectrogram.</li> <li><strong>Decoder</strong>: Transpose convolutions that reconstruct the spectrogram from encoded features.</li> <li><strong>Skip Connections</strong>: Feature maps from encoder layers are concatenated with corresponding decoder layers to preserve spatial information.</li> </ol> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/speech-enhance/complexskipnet-480.webp 480w,/assets/img/speech-enhance/complexskipnet-800.webp 800w,/assets/img/speech-enhance/complexskipnet-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/speech-enhance/complexskipnet.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Real Valued Skip U-Net from Kothapally, V. et al. 2020" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <label for="output-audio-example">Diagram from Kothapally, V. et al. (2020)</label> </div> </div> <p>My design extends the real-valued U-Net for speech dereverberation design proposed by Kothapally, V. et al. (2020) to the complex domain. It should be noted that my original inspiration for this combination of ideas was from Choi, H.-S. et al. (2019), although the author redacted the paper due to flaws in their methodology.</p> <h3 id="real-valued-skip-u-net-vs-complex-u-net">Real-Valued Skip U-Net vs. Complex U-Net</h3> <ul> <li><strong>Real-Valued U-Net</strong>: Operates only on magnitude spectrograms, combining the output with the noisy phase.</li> <li><strong>Complex U-Net</strong>: Processes the full complex STFT coefficients, preserving phase information.</li> </ul> <p><img src="assets/real_valued_skip_unet.png" alt="Real-Valued Skip U-Net Architecture"/> <img src="assets/complex_conv_layer.png" alt="Complex Convolutional Layer"/></p> <hr/> <h2 id="post-processing-with-signal-processing">Post-Processing with Signal Processing</h2> <p>The U-Net applies a tanh activation function to the complex masking coefficients, which stabilizes the output but imposes a strict bound on the signal’s intensity. This often results in a reduced perceived loudness. To address this, I designed a custom loudness normalization algorithm that adjusts the output’s amplitude envelope, ensuring it closely matches the input’s amplitude envelope without any coefficients exceeding the input’s intensity.</p> <p>The key steps are:</p> <ol> <li><strong>STFT Magnitude Adjustment</strong>: <ul> <li>Compute scaling factors for each time-frequency coefficent based on the ratio of the magnitude spectrogram of the input signal to the magnitude spectrogram of the output signal.</li> </ul> </li> <li><strong>Normalization Formula</strong>: <ul> <li>If the network reduced the magnitude of a coeffient in the output relative to the min and max of the input and output spectrograms, we do not boost the magnitude of those coefficents</li> </ul> </li> <li><strong>Reconstruction</strong>: <ul> <li>Apply the inverse STFT (ISTFT) to the normalized audio to generate the final time-domain signal.</li> </ul> </li> </ol> <p>More formally, let \(X\) be the STFT of the input signal, \(Y\) be the STFT of the output signal. Let \(|X|^{\text{norm}}\) and \(|Y|^{\text{norm}}\) be the min-max normalized spectrograms of \(X\) and \(Y\), respectively.</p> <p>The scaling factor \(sf_{ij}\) for each complex coefficient \(Y_{ij}\) is defined as:</p> \[\text{sf}_{ij} = \begin{cases} 1 &amp; \text{if } \frac{|X_{ij}|^{\text{norm}}}{|Y_{ij}|^{\text{norm}}} &lt; 1 \\ \frac{|X_{ij}|}{|Y_{ij}|} &amp; \text{otherwise} \end{cases}\] <p>This scaling factor adjusts the magnitude of \(Y_{ij}\) to match the desired relationship between the input and output signals. In the first case of the of the piecewise function, we assume the network tried to reduce the contribution of this coefficient to the strength of signal, so we do not apply the scaling factor. If this ratio is greater than or equal to one, the scaling factor is applied to the complex coefficient.</p> <p>Finally, to limit the case where the scaling factor leads to unpleasantly large increases in energy, the scaling factor is limited s.t. the magnitude of each coefficient can’t surpass the magnitude of the input coefficient.</p> <p>This method assumes that the input coefficients with useful content always have greater magnitudes than the corresponding time-frequency bins in the output signal. In cases where the input signal is extremely faint, the network’s output can become nearly inaudible without the post-processing step. Handling this specific edge case has been left for future work.</p> <h2 id="results-and-recap">Results and Recap</h2> <p>Overall, I found that the network improved the average PESQ compared to the noisy examples it was trained on. While the network’s performance on real-world data indicates there is room for improvement, the results were promising given the limited data and compute available. It was exciting to see that the network could generalize to some extent.</p> <p>In the future, I would like to extend the training time, increase the data set, and introduce learnable parameters to the signal processing operations. Since differentiable DSP operations can be integrated directly into neural pipelines, this could further enhance the quality of the output.</p> <hr/> <h3 id="assets">Assets</h3> <ul> <li><a href="assets/input_output_spectrogram.png">Input and Output Spectrograms</a></li> <li><a href="assets/audio_example.mp3">Audio Example (Input and Output)</a></li> </ul>]]></content><author><name></name></author><category term="Machine-Learning"/><category term="Audio-Processing"/><category term="speech"/><category term="accessibility"/><category term="signal-processing"/><summary type="html"><![CDATA[Noisy environments can make speech more difficult to understand. In this project I explore a novel method for removing background noise and echos for audio files with multiple speakers]]></summary></entry><entry><title type="html">Experimental Design Strategies for Engaging Quantitative Customers</title><link href="https://n-reeves.github.io/blog/2025/experiment-design-for-quantitative-customers/" rel="alternate" type="text/html" title="Experimental Design Strategies for Engaging Quantitative Customers"/><published>2025-01-22T11:59:00+00:00</published><updated>2025-01-22T11:59:00+00:00</updated><id>https://n-reeves.github.io/blog/2025/experiment-design-for-quantitative-customers</id><content type="html" xml:base="https://n-reeves.github.io/blog/2025/experiment-design-for-quantitative-customers/"><![CDATA[<p>The write up on this page is in progress. Please check back in a week.</p> <p>The first version of my article on <a href="https://n-reeves.github.io/blog/2025/experiment-design-customer-service/">experiment design for service tecniques</a> is live if you would like a reference on my approach to experiment design.</p>]]></content><author><name></name></author><category term="Statistics"/><summary type="html"><![CDATA[Good experiments can help your sales and marketing teams find common ground with your customers. This article covers the value of observational studies in driving growth in a technical customer base]]></summary></entry><entry><title type="html">Experimental Design for Call Center Support Strategies</title><link href="https://n-reeves.github.io/blog/2025/experiment-design-customer-service/" rel="alternate" type="text/html" title="Experimental Design for Call Center Support Strategies"/><published>2025-01-21T11:59:00+00:00</published><updated>2025-01-21T11:59:00+00:00</updated><id>https://n-reeves.github.io/blog/2025/experiment-design-customer-service</id><content type="html" xml:base="https://n-reeves.github.io/blog/2025/experiment-design-customer-service/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In modern customer service, call centers play a critical role in shaping the customer experience and, by extension, customer loyalty. As businesses strive to improve these experiences, they often experiment with new customer service methods or tools to identify approaches that work best. But without a robust statistical framework, assessing these changes becomes prone to bias and misinterpretation, potentially leading to costly missteps.</p> <p>Randomized Control Trials (RCTs) offer a powerful methodology for evaluating new strategies in call centers. By randomizing treatments and analyzing their outcomes, businesses can identify causality with confidence, even when dealing with complex systems like human behavior and operational variability. This article provides a structured approach to designing, analyzing, and interpreting experiments in call center environments, emphasizing the importance of statistical rigor in business decision-making.</p> <p><em>Disclaimer on enerative AI use:</em> This document was created with the help of ChatGPT, which summarized key content from a larger unstructured piece I wrote on the topic. While I’ve validated the content in the article, this page will change a lot over the next week as I edit and expand the content here.</p> <h2 id="why-statistics">Why Statistics?</h2> <p>Businesses invest heavily in customer service because it directly impacts retention, reputation, and revenue. Despite this, decision-makers often rely on anecdotal evidence or incomplete data to assess the success of new initiatives. This can lead to ineffective strategies being implemented at scale or valuable ones being overlooked.</p> <p>Statistics provide a way to objectively measure the impact of changes, isolating the effects of a new customer service method from other confounding factors like seasonal trends, customer demographics, or operational differences. For example, while a manager might assume a drop in call resolution time indicates an improvement, statistical analysis can reveal whether this change is consistent across representative groups or if it disproportionately benefits a subset of customers.</p> <p>By applying a rigorous statistical framework, companies can evaluate whether a new approach genuinely improves customer experience or simply shifts the burden elsewhere.</p> <h2 id="experimental-design">Experimental Design</h2> <h3 id="randomized-control-trials-rcts-principles-and-advantages">Randomized Control Trials (RCTs): Principles and Advantages</h3> <p>RCTs are the gold standard for causal inference because they minimize bias through random assignment. In a call center, this could involve randomly assigning callers to different customer service methods, ensuring each group is comparable.</p> <p>This randomness helps account for external factors like time of day, caller mood, or even weather, which might otherwise skew results. For example, callers during peak hours may have different expectations than those during quieter times, making randomization critical for isolating the effect of the new method.</p> <h3 id="challenges-unique-to-call-centers">Challenges Unique to Call Centers</h3> <p>Call centers introduce unique challenges, such as variability among agents, geographic diversity of callers, and the difficulty of obtaining consistent feedback. Designing experiments that accommodate these factors—like stratifying samples by geography or time of day—ensures more reliable results.</p> <h2 id="measuring-customer-experience">Measuring Customer Experience</h2> <p><strong>Response Variables: Defining Success</strong> The choice of response variable is critical. Common metrics include customer satisfaction scores, first-call resolution rates, and net promoter scores (NPS). However, these metrics come with caveats:</p> <ul> <li>Optional ratings: When responses are voluntary, they often exhibit a bimodal distribution—extremely high or low ratings—making it harder to detect subtle changes in customer sentiment.</li> <li>Ordinal data: Metrics like 1–5 ratings are ordinal, not interval, meaning the difference between a 4 and a 5 may not be equivalent to that between a 2 and a 3.</li> </ul> <p>To mitigate these issues, businesses can:</p> <ul> <li>Standardize feedback forms and encourage consistent participation</li> <li>Use binary variables (e.g., “positive” vs. “negative” experiences) for initial analysis</li> <li>Explore models that leverage the ordinal nature of the variable</li> </ul> <h2 id="addressing-common-problems">Addressing Common Problems</h2> <h3 id="sampling-strategies">Sampling Strategies</h3> <p>Sampling can introduce bias if not carefully managed. For instance, selecting only a handful of agents to implement a new strategy may result in correlated data due to agent-specific effects. A better approach is stratified random sampling, ensuring all relevant subgroups (e.g., geographic regions or age brackets) are represented.</p> <h3 id="correlation-between-observations">Correlation between Observations</h3> <p>In call center experiments, hierarchical models are invaluable for accounting for nested data structures. For example:</p> <ul> <li>Fixed effects could include time of day or customer demographics.</li> <li>Random effects could capture variability among agents, accounting for the fact that some are naturally more effective than others.</li> </ul> <p>By modeling this hierarchy, businesses can better understand the true impact of a new method, separating agent-level effects from the overall treatment effect.</p> <h2 id="case-study-applying-the-framework">Case Study: Applying the Framework</h2> <p>An RCT comparing two customer service methods might proceed as follows:</p> <ul> <li><strong>Setup:</strong> 10 agents are trained in both methods, and 2,000 callers are randomly assigned to each method over a month.</li> <li><strong>Analysis:</strong> Hierarchical logistic regression reveals that the new method increases the odds of a positive review by 20%, with no significant increase in negative reviews.</li> </ul> <p>This result provides a clear, data-driven rationale for scaling the new method while addressing potential stakeholder concerns about negative impacts.</p> <h2 id="communication-and-pragmitism-in-applied-statistics">Communication and Pragmitism in Applied Statistics</h2> <p>Statistical findings must be translated into actionable insights for business leaders. For example:</p> <ul> <li>Use visualizations like confidence intervals to convey uncertainty.</li> <li>Avoid jargon like “p-values” unless paired with intuitive explanations (e.g., “There’s less than a 5% chance these results are due to random chance”).</li> </ul> <p>Pragmatism is key: While statistical rigor is essential, businesses ultimately need to make decisions. Even imperfect data can guide better choices if interpreted thoughtfully.</p> <h2 id="future-directions-ai-llms-and-the-future-of-service">Future Directions: AI, LLMS, and the Future of Service</h2> <p>As AI tools like large language models (LLMs) become integral to customer service, the role of statistics becomes even more critical. A/B testing AI-driven methods—such as comparing chatbot versions or evaluating hybrid human-AI workflows—requires robust frameworks to measure their impact on customer experience.</p> <p>Automating these frameworks ensures consistency and scalability, allowing businesses to rapidly iterate on new tools while maintaining a clear link between service strategies and customer satisfaction.</p>]]></content><author><name></name></author><category term="Statistics"/><summary type="html"><![CDATA[Call center support is a fundamental component of the customer experience in any organization. This article details an approach I have used to quantify the impact of new support initiatives on the customer experience before they launch]]></summary></entry></feed>