<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Experimental Design for Call Center Support Strategies | Nicholas W. Reeves </title> <meta name="author" content="Nicholas W. Reeves"> <meta name="description" content="Call center support is a fundamental component of the customer experience in any organization. This article details an approach I have used to quantify the impact of new support initiatives on the customer experience before they launch"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://n-reeves.github.io/blog/2025/experiment-design-customer-service/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Nicholas</span> W. Reeves </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/resume/">Resume </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Experimental Design for Call Center Support Strategies</h1> <p class="post-meta"> Created in January 21, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/category/statistics"> <i class="fa-solid fa-tag fa-sm"></i> Statistics</a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#introduction">Introduction</a></li> <li class="toc-entry toc-h2"><a href="#why-statistics">Why Statistics?</a></li> <li class="toc-entry toc-h2"> <a href="#experimental-design">Experimental Design</a> <ul> <li class="toc-entry toc-h3"><a href="#randomized-control-trials-rcts-principles-and-advantages">Randomized Control Trials (RCTs): Principles and Advantages</a></li> <li class="toc-entry toc-h3"><a href="#challenges-unique-to-call-centers">Challenges Unique to Call Centers</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#measuring-customer-experience">Measuring Customer Experience</a></li> <li class="toc-entry toc-h2"> <a href="#addressing-common-problems">Addressing Common Problems</a> <ul> <li class="toc-entry toc-h3"><a href="#sampling-strategies">Sampling Strategies</a></li> <li class="toc-entry toc-h3"><a href="#assigning-treatment">Assigning Treatment</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#testing">Testing</a> <ul> <li class="toc-entry toc-h3"><a href="#mixed-effects-hierarchical-models">Mixed Effects (Hierarchical) Models</a></li> <li class="toc-entry toc-h3"><a href="#mixed-effects-models-within-the-framework">Mixed Effects Models within the Framework</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#case-study-applying-the-framework">Case Study: Applying the Framework</a> <ul> <li class="toc-entry toc-h3"><a href="#set-up">Set Up</a></li> <li class="toc-entry toc-h3"><a href="#variables-and-model-definition">Variables and Model Definition</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#communication-and-pragmitism-in-applied-statistics">Communication and Pragmitism in Applied Statistics</a></li> <li class="toc-entry toc-h2"><a href="#future-directions-ai-llms-and-the-future-of-service">Future Directions: AI, LLMS, and the Future of Service</a></li> </ul> </div> <hr> <div id="markdown-content"> <h2 id="introduction">Introduction</h2> <p>In modern customer service, call centers play a critical role in shaping the customer experience and, by extension, customer loyalty. As businesses strive to improve these experiences, they often experiment with new customer service methods or tools to identify approaches that work best. Without a robust statistical framework, assessing these changes becomes prone to bias and misinterpretation, potentially leading to costly missteps or the adoption of innefective programs.</p> <p>Randomized Control Trials (RCTs) offer a powerful methodology for evaluating new strategies in call centers. By randomizing treatments and analyzing their outcomes, businesses can identify causality with confidence, even when dealing with complex systems like human behavior and operational variability. Additionally, as agent-based AI systems are increasingly adopted by service organizations, robust frameworks for assessing their performance are essential to guiding development. Effective statistical models can bridge the gap between the machine learning development lifecycle and proxies for the customer experience</p> <p><em>Disclaimer:</em> This article is in progress. Updates may flow in throughout the week</p> <h2 id="why-statistics">Why Statistics?</h2> <p>Businesses invest heavily in customer service because it directly impacts retention, reputation, and revenue. Despite this, decision-makers often rely on anecdotal evidence or incomplete data to assess the success of new initiatives. This can lead to ineffective strategies being implemented at scale or valuable ones being overlooked.</p> <p>Statistics provide a way to objectively measure the impact of changes, isolating the effects of a new customer service method from other confounding factors like seasonal trends, customer demographics, or operational differences. For example, while a manager might assume a drop in call resolution time indicates an improvement, statistical analysis can reveal whether this change is consistent across representative groups or if it disproportionately benefits a subset of customers.</p> <p>By applying a rigorous statistical framework, companies can evaluate whether a new approach genuinely improves customer experience or simply shifts the burden elsewhere.</p> <h2 id="experimental-design">Experimental Design</h2> <h3 id="randomized-control-trials-rcts-principles-and-advantages">Randomized Control Trials (RCTs): Principles and Advantages</h3> <p>RCTs are the gold standard for causal inference because they minimize bias through random assignment. In a call center, this could involve randomly assigning callers to different customer service methods, ensuring each group is comparable.</p> <p>This randomness helps account for external factors like time of day, caller mood, or even weather, which might otherwise skew results. For example, callers during peak hours may have different expectations than those during quieter times, making randomization critical for isolating the effect of the new method.</p> <h3 id="challenges-unique-to-call-centers">Challenges Unique to Call Centers</h3> <p>Call centers introduce unique challenges, such as variability among agents, geographic diversity of callers, and the difficulty of obtaining consistent feedback. Designing experiments that accommodate these factors—like stratifying samples by geography or time of day—ensures more reliable results.</p> <p>Additionally, it can be expensive and time-consuming to train your representatives on a new approach to service. These costs can increase the risk of altering a service strategy without solid evidence to support their value.</p> <h2 id="measuring-customer-experience">Measuring Customer Experience</h2> <p><strong>Response Variables: Defining Success</strong> The choice of response variable is critical. Common metrics include customer satisfaction scores, first-call resolution rates, and net promoter scores (NPS). However, these metrics come with caveats:</p> <ul> <li>Optional ratings: When responses are voluntary, they often exhibit a bimodal distribution—extremely high or low ratings—making it harder to detect subtle changes in customer sentiment.</li> <li>Ordinal data: Metrics like 1–5 ratings are ordinal, not interval, meaning the difference between a 4 and a 5 may not be equivalent to that between a 2 and a 3.</li> </ul> <p>To mitigate issues with response rate, businesses can:</p> <ul> <li>Integrate feedback collection directly into the call with the representative</li> <li>For digital products, work with product teams to integrate the feedback form directly into the product itself.</li> </ul> <p>An important to consider that feedback questions asked directly representatives may result in skewed answers. Many people will feel more uncomfortable giving negative feedback when a human is involved.</p> <p>To handle missing response variables and the bimodal nature of the data, some solutions are:</p> <ul> <li>Opt for measures of call resolution rates that don’t depend on optional customer feedback.</li> <li>Use binary variables to categorize experiences into positive/not-positive or negative/not-negative.</li> </ul> <p>It’s important to note that there are often underlying relationships that influence the presence and absence of responses. It is always possible that highly positive or negative experiences may lead to higher response rates. Additionally, while some may wish to take advantage of the ordinal nature of the data, it’s worth reflecting on what each category means and what benefit you hope to gain in inference by adopting these techniques. In practice, I have found it helpful to reduce the problem to measures that identify when customers have positive experiences and when they have negative experiences.</p> <h2 id="addressing-common-problems">Addressing Common Problems</h2> <h3 id="sampling-strategies">Sampling Strategies</h3> <p>Sampling can introduce bias if not carefully managed. When measuring call center performance, I have two primary considerations. First, are our key customer demographics represented fairly in the data we collect? Second, what is the relationship between the skill of individual reps and the customer experience?</p> <p>In an ideal world, we want to adopt service strategies that improve the experience for every possible group. In practice, you may want to prioritize certain groups. Getting equal representation of your customers across geography, age, gender, etc., is difficult without mature service analytics infrastructure. Stratified sampling is a great way to get the desired effect but relies on a link between the calls and user demographic data.</p> <p>Additionally, the time when users call is also an important consideration and worth stratifying depending on your product. A user that calls in at three in the morning is likely to have a pretty good reason to do so. There may be a relationship between this variable and the customers that need the highest quality of care. Without consideration, it’s possible that peak time calls are overrepresented in your experiment, reducing the power of inference.</p> <p>On the second point, good reps can have a large positive effect on the way your customers feel after calling in and resolution rates. Naturally, a large number of calls and reps can help control for an individual’s influence on the way you measure a new strategy. However, this would defeat the purpose of this framework. We want to avoid investing money into training a large number of representatives on a new strategy without evidence that supports it’s an improvement. This problem can be addressed through a combination of how we randomly assign treatment and the statistical techniques that we use.</p> <h3 id="assigning-treatment">Assigning Treatment</h3> <p>To randomly assign a new service technique, start by selecting a subset of service representatives at random. Using stratified sampling can ensure their hours and locations cover the temporal and geographic ranges you wish to control for. Over time, calls handled by these representatives will form the treatment group. To account for ramp-up or training, you may exclude data from the initial days or weeks.</p> <p>This approach minimizes the training required and reduces the risk of exposing customers to a suboptimal process. Alternatively, representatives could alternate between strategies for each call, limiting the analysis to this group. While this controls for individual differences, there are practical and theoretical drawbacks:</p> <ul> <li>The method doesn’t leverage the full dataset from your organization.</li> <li>It requires technical infrastructure that may not always be feasible.</li> <li>Alternating strategies could confuse representatives, potentially lowering service quality.</li> </ul> <p>As I’ll cover in the next section, an appropriate set of statistical models/tests can help account for the potential correlation between calls accepted by the same rep and the influence of an individual rep’s skill level on the response.</p> <h2 id="testing">Testing</h2> <h3 id="mixed-effects-hierarchical-models">Mixed Effects (Hierarchical) Models</h3> <p>Hierarchical data is prevalent in business settings, and recognizing it is crucial for improving the quality of inference. For example, you might work with user-level data to analyze webpage clicks but have multiple observations for each customer. In clinical settings, you may focus on patient outcomes while accounting for the hospital. Other examples include test scores grouped by students or shipping speed influenced by warehouses. Such cases can create challenges for standard regression analysis and hypothesis testing, as including all data risks introducing correlation between observations.</p> <p>Mixed effects models, also known as hierarchical or multilevel models, combine fixed and random effects to analyze data with a naturally grouped structure (e.g., customer data grouped by region or product). They help model relationships more accurately when there are varying influences across subgroups within a larger population.</p> <ul> <li>Fixed effects are coefficients you want to estimate and interpret directly, representing the average relationship across all data.</li> <li>Random effects capture subgroup-level variability and account for differences between groups, using partial pooling to balance the influence of smaller and larger groups.</li> </ul> <p>This article doesn’t serve as a full explanation of the theory behind these models. If you would like a great resource, <a href="https://bookdown.org/roback/bookdown-BeyondMLR/" rel="external nofollow noopener" target="_blank">Beyond Multiple Linear Regression</a> by Roback and Legler is a great textbook that provides practical advice on multi-level modeling in R. As a high-level reference, it’s important to know the following:</p> <ul> <li>Generalized linear models assume that observations are independent of one another.</li> <li>Hierarchical data risks violating this assumption.</li> <li>You may have many groups, or the size of your groups may vary significantly. In either case, this could result in issues when applying standard encoding methods for categorical variables.</li> <li>Mixed effects models address these issues by treating group-level effects as random variables. This approach allows the model to learn from all groups collectively, applying partial pooling to shrink group estimates toward the overall population mean.</li> </ul> <p>The result is a robust inferential tecnique that allows you to:</p> <ul> <li>Measure the relationship between your predictors (e.g., treatments) and a response variable while accounting for correlation within groups.</li> <li>Quantify how much variability in the response can be attributed to differences between groups, offering insight into the influence of group-level factors on the aggregate response. (How much of the total variance in customer feedback is explained by an individual rep’s skill?)</li> </ul> <h3 id="mixed-effects-models-within-the-framework">Mixed Effects Models within the Framework</h3> <p>Mixed effects models are well suited to this experimental framework, as they enable the analysis of hierarchical data while addressing the variability inherent in call center environments. These models combine fixed effects, which capture the treatment impact of a new service strategy, with random effects that account for differences among individual customer service representatives.</p> <p>Depending on the context, additional variables can be modeled as fixed or random effects. Fixed effects represent factors of primary interest that remain consistent across repeated studies, while random effects account for variations arising from sampling. For example, in our service study, the selection of representatives to implement the new technique and the geographic distribution of callers could be modeled as random effects. Conversely, the service strategy type, which is central to understanding its causal impact on the response, would be treated as a fixed effect.</p> <h2 id="case-study-applying-the-framework">Case Study: Applying the Framework</h2> <h3 id="set-up">Set Up</h3> <p>For ease of understanding, let’s walk through a theoretical experiment. Imagine you are interested in releasing a new AI agent that integrates with the process used by your service reps. You start by randomly sampling 1,000 reps across your different call centers. You then partition these reps into groups based on when they work and their geographic coverage. Using stratified sampling, you produce a set of 100 reps that are evenly distributed across the time frames and regions you are concerned with. These reps are trained in the new technique and begin to apply it in practice.</p> <p>To eliminate potential confounding effects from ramp-up time, you exclude the first three days of data after training. Your call centers ensure a high match rate with internal customer demographic data by using robust customer ID matching based on phone numbers and names recorded during calls.</p> <p>The experiment includes calls handled by 2,000 representatives, among them 100 trained reps using the new AI agents. The objective is to determine whether the new AI agents have a positive causal effect on a binary response: whether customers rated their experience with four or five stars on a 1–5 scale in an optional questionnaire embedded in your product. However, you suspect that late-hour reps may encounter more irate customers and that older customers are more likely to complete the questionnaire, both of which could bias the results.</p> <h3 id="variables-and-model-definition">Variables and Model Definition</h3> <p>Let \(Y_{ij}\) represent the response variable for the \(i\)th representative on their \(j\)th call. Let \(T_{i}\) be the binary variable indicating whether representative \(i\) was trained on and applied the AI-based strategy. Let \(B_{ij}\) be a binary variable indicating whether the customer on the $i$th rep’s $j$th call is over the age of 70, and let \(C_{ij}\) be a binary variable indicating whether the call occurred between 12 a.m. and 7 a.m. in the customer’s local timezone.</p> <p>In regression analysis for inference, it is common to start by building a model without the variable of interest. Once the model is complete, the treatment variable is added to assess its significance. For simplicity, we define the entire model, including the treatment variable, from the outset.</p> <p>Call level (level one):</p> <p>\begin{equation} Y_{ij} ​= a_{i} + b_{i}B_{ij} + c_{i}C_{ij} + \epsilon_{ij} \end{equation}</p> <p>Here, \(a_{i}\) represents the representative-specific intercept, \(b_{i}\) represents the representative-specific effect of working with older callers, and \(c_{i}\) represents the representative-specific effect of taking nighttime calls. \(\epsilon_{ij}\) is the randomly distributed error associated with the observation.</p> <p>Rep level (level two):</p> <p>\begin{equation} a_{i} = \alpha_{0} + \alpha_{1}T_{i} + u_{i} \end{equation} \begin{equation} b_{i} = \beta_{0} + \beta_{1}T_{i} + v_{i} \end{equation} \begin{equation} c_{i} = \gamma_{0} + \gamma_{1}T_{i} + w_{i} \end{equation}</p> <p>The representative-level variables are defined based on the interaction terms with the treatment variable that we include in our model. The key idea is that \(u_{i}, v_{i}, w_{i}\) are random effects intended to capture between-rep variability in the intercepts and slopes. The coefficients \(\alpha, \beta,\gamma\) are all fixed effects. If the new strategy has a positive relationship with the response, we would expect this to be reflected in the values and significance of \(\alpha_{1}, \beta_{1}, \gamma_{1}\).</p> <h2 id="communication-and-pragmitism-in-applied-statistics">Communication and Pragmitism in Applied Statistics</h2> <p>When working with business leaders, simply stating the p-value, confidence interval, and point estimate associated with the coefficient of your treatment is not enough. Extending these estimations to provide a range of expected ROI or other actionable metrics in the context of the response variable is critical. Additionally, incorporating the cost of implementing the program can make the decision easier and facilitate a more productive discussion.</p> <p>To adjust and simplify our case study, suppose we adopted a simpler model that dropped all interaction terms with the treatment, leaving only a fixed effect associated with it. Assume our response variable is binary, indicating whether the caller gave a 4+ star rating.</p> <p>Suppose the estimated coefficient for the treatment is \(1.25\) with a \(95%\) confidence interval of \([1.1, 1.4]\). Thus, the new strategy leads to an increase in the log odds by \(1.25\). But what does this mean? We can exponentiate the bounds of the confidence interval to instead produce the estimates in terms of the odds (the ratio of the probability of success to the probability of failure). The result is a confidence interval that spans \([~3, ~4]\), but again, how can this be used to help business decisions?</p> <p>One straightforward method is to tie the results of inference back to the KPI that motivated your choice in response. For example, an org might use the percentage of calls with a certain rating as a health check. Look at the current rate of positive ratings under the existing strategy to contextualize what a three to four times increase in the probability of receiving a positive rating would look like.</p> <p>Say the current rate is \(.4\) Under the assumption that \(p = .4\), the existing odds would be \(\frac{.4}{.6} = \frac{40}{60}\)</p> <p>Convert the bounds of your confidence intervals to odds based on your existing success rate:</p> <p>Lower Bound: \(40/60 * 3 = 120/60 = 2\)</p> <p>Upper Bound: \(40/60 * 4 = 160/60 = 2.67\)</p> <p>Now convert the odds back to probabilities to produce an estimated range for the new rate:</p> <p>Lower bound: \(\frac{2}{1 + 2} = .67\)</p> <p>Upper Bound: \(\frac{2}{1 + 2.67} = .73\)</p> <p>To contextualize the confidence interval, remind stakeholders that these values represent the range of plausible outcomes given the data. I like to provide extreme examples to help them understand the downside of working with point estimates alone.</p> <ul> <li>If customers assigned to trained reps were unusually cheerful by chance, the results might overestimate the true effect.</li> <li>Conversely, if treated reps happened to get more difficult customers, the results might underestimate the effect.</li> </ul> <p>The confidence interval is just a means to quantify this effect. If we were to repeat this experiment 100 times with random sets of customers and random reps each time, we would expect that in \(95%\) of these trials this range would include the true value added by the new strategy. If this explanation is too heady for your partners, you can start by explaining there is a \(95%\) chance that the value offered by the new strategy will fall between the range that you calculated.</p> <h2 id="future-directions-ai-llms-and-the-future-of-service">Future Directions: AI, LLMS, and the Future of Service</h2> <p>As AI tools like large language models (LLMs) become integral to customer service, the role of statistics becomes even more critical. A/B testing AI-driven methods—such as comparing chatbot versions or evaluating hybrid human-AI workflows—requires robust frameworks to measure their impact on customer experience.</p> <p>Automating these frameworks ensures consistency and scalability, allowing businesses to rapidly iterate on new tools while maintaining a clear link between service strategies and customer satisfaction.</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Nicholas W. Reeves. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>